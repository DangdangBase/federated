# Copyright 2022, Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r"""This is the python binary of federated training embedding models.

Local debug command:

bazel run run_federated -- \
  --task_type=emnist --total_rounds=3 --clients_per_round=2 \
  --rounds_per_eval=1 --max_examples_per_client=8 --client_batch_size=8
"""
import collections
from collections.abc import Sequence
import functools
import os
import time
from typing import Union

from absl import app
from absl import flags
from absl import logging

from dp_visual_embeddings import trainer
from dp_visual_embeddings.models import build_model
from dp_visual_embeddings.tasks import build_task
from dp_visual_embeddings.utils import file_utils


IRRELEVANT_FLAGS = frozenset(iter(flags.FLAGS))

# Experiment configuration
_EXPERIMENT_NAME = flags.DEFINE_string(
    'experiment_name', 'federated_embedding',
    'The name of this experiment. Will be'
    'append to  --root_output_dir to separate experiment results. This name is'
    'often generated by XM script for each running task.')
_ROOT_OUTPUT_DIR = flags.DEFINE_string(
    'root_output_dir', '/tmp/', 'Root directory for writing experiment output.')

# Runtime configuration
_MAX_CONCURRENT_THREADS = flags.DEFINE_integer(
    'max_concurrent_threads', None,
    'The maximum number of concurrent calls to a single computation in the TFF '
    'CPP runtime. It is often used to avoid OOM on GPU for large number of '
    'clients, and avoid parallelism bottleneck. If `None`, there is no limit.')
_REPEAT = flags.DEFINE_integer('repeat', None, 'Repeat index.')

# Training configuration
_TASK_TYPE = flags.DEFINE_enum(
    'task_type', None,
    [task_type.name.lower() for task_type in build_task.get_task_types()],
    'Which task (dataset and model) to use in the experiment.')
_TOTAL_ROUNDS = flags.DEFINE_integer('total_rounds', 10,
                                     'Number of total training rounds.')
_CLIENTS_PER_ROUND = flags.DEFINE_integer(
    'clients_per_round', 10, 'How many clients to train on per round.')
_ROUNDS_PER_EVAL = flags.DEFINE_integer(
    'rounds_per_eval', 2,
    'Evaluate the trained model on the validation dataset every '
    '`rounds_per_eval` rounds.')
_ROUNDS_PER_CHECKPOINT = flags.DEFINE_integer(
    'rounds_per_checkpoint', 10,
    'Save checkpoints of server state every `rounds_per_checkpoint` rounds.')
_ROUNDS_PER_EXPORT = flags.DEFINE_integer(
    'rounds_per_export', 200,
    'Period at which to write an exported SavedModel.')
_MAX_EXAMPLES_PER_CLIENT = flags.DEFINE_integer(
    'max_examples_per_client', None,
    'Max number of training samples to use per client.'
    'Default to None, which will use all samples per client.')
_CLIENT_BATCH_SIZE = flags.DEFINE_integer('client_batch_size', 16,
                                          'Batch size used on the client.')
_EVAL_BATCH_SIZE = flags.DEFINE_integer('eval_batch_size', 256,
                                        'Batch size used for online eval.')
_CLIENT_SHUFFLE_BUFFER_SIZE = flags.DEFINE_integer(
    'client_shuffle_buffer_size', None,
    'Size of buffer used to shuffle examples in client data '
    'input pipeline. No shuffling will be done if 0. The '
    'default, if None, is `10 * client_batch_size`.')
_CLIENT_EPOCHS_PER_ROUND = flags.DEFINE_integer(
    'client_epochs_per_round', 1,
    'Number of epochs in the local client computation per round.')

# Federated specific
_USE_CLIENT_SOFTMAX = flags.DEFINE_boolean(
    'use_client_softmax', True, 'Whether to use client-based mapping to '
    'generate labels from identities for training. It is used to implement'
    'client-sampled softmax.')
_DYNAMIC_CLIENTS = flags.DEFINE_integer(
    'dynamic_clients', 1,
    'Dynamically merge clients to super clients for user-level DP.')

_MODEL_BACKBONE = flags.DEFINE_enum(
    'model_backbone', 'mobilenet2', build_model.get_backbone_names(),
    'Selects between different convolutional "backbones:" the main part of the '
    'model.')

_PRETRAIN_MODEL_PATH = flags.DEFINE_string(
    'pretrain_model_path', None,
    'Path to a pre-trained model of SavedModel format, which will be loaded '
    'by `tf.keras.models.load_model`. If None, no pretrained model is used.')
_TRAIN_CONV = flags.DEFINE_boolean(
    'trainable_conv', True, 'Whether to train the weights of convolutional '
    'layers. Can be used to reduce the size of trainable parameters when set '
    'to False.')

# Optimizers
_SERVER_LR = flags.DEFINE_float('server_lr', 0.1, 'Server learning rate.')
_SERVER_MOMENTUM = flags.DEFINE_float('server_momentum', 0.9,
                                      'Server momentum.')
_CLIENT_LR = flags.DEFINE_float('client_lr', 0.01, 'Client learning rate.')
_CLIENT_MOMENTUM = flags.DEFINE_float('client_momentum', 0, 'Client momentum.')
_CLIENT_OPTIMIZER = flags.DEFINE_enum('client_optimizer', 'sgd',
                                      ['sgd', 'adam'],
                                      'Type of the client optimizer.')
_HEAD_LR_SCALE = flags.DEFINE_float(
    'head_lr_scale', 1,
    'Use head_lr_scale*client_lr as learning rate to update the local variables'
    ' (head of the model).')
_RECONST_ITERS = flags.DEFINE_integer(
    'reconst_iters', None,
    'If not `None`, first optimize the head of the embedding model before '
    'training the Backbone. Usually can set to half of the training iterations,'
    'e.g., max_examples_per_client/client_batch_size.')

# Aggregator / DP
_AGG_TYPE = flags.DEFINE_enum(
    'aggregator_type', None,
    [agg_type.name.lower() for agg_type in trainer.get_aggregator_types()],
    'Aggregator type: None, or dpsgd, or dpftrl.')
_UNCLIP_QUANTILE = flags.DEFINE_float(
    'unclip_quantile', None,
    'Target quantile for adaptive clipping. If `None`, use fixed clipping.')
_CLIP_NORM = flags.DEFINE_float('clip_norm', None, 'Clip L2 norm.')
_NOISE_MULTIPLIER = flags.DEFINE_float('noise_multiplier', None,
                                       'Noise multiplier for DP algorithm.')


HPARAM_FLAGS = [f for f in flags.FLAGS if f not in IRRELEVANT_FLAGS]
FLAGS = flags.FLAGS


def _write_hparam_flags():
  """Save hyperparameter flags to be used together with saved metrics."""
  logging.info('Show FLAGS for debugging:')
  for f in HPARAM_FLAGS:
    logging.info('%s=%s', f, FLAGS[f].value)
  hparam_dict = collections.OrderedDict([
      (name, FLAGS[name].value) for name in HPARAM_FLAGS
  ])
  results_dir = os.path.join(_ROOT_OUTPUT_DIR.value, 'results',
                             _EXPERIMENT_NAME.value)
  file_utils.create_if_not_exists(results_dir)
  hparam_file = os.path.join(results_dir, 'hparams.csv')
  file_utils.atomic_write_series_to_csv(hparam_dict, hparam_file)


def _check_positive(value: Union[int, float]):
  if value <= 0:
    raise ValueError(f'Got {value} for positive input.')


def main(argv: Sequence[str]) -> None:
  if len(argv) > 1:
    raise app.UsageError('Expected no command-line arguments, '
                         'got: {}'.format(argv))
  logging.info('Start training...')
  _write_hparam_flags()

  task_type_str = _TASK_TYPE.value
  if task_type_str is None:
    raise app.UsageError('Must specify --task_type')
  else:
    task_type = build_task.TaskType[task_type_str.upper()]

  model_backbone = build_model.ModelBackbone[_MODEL_BACKBONE.value.upper()]

  start_time = time.time()
  task = build_task.configure_task(
      task_type=task_type,
      client_batch_size=_CLIENT_BATCH_SIZE.value,
      eval_batch_size=_EVAL_BATCH_SIZE.value,
      train_max_examples_per_client=_MAX_EXAMPLES_PER_CLIENT.value,
      eval_max_examples_per_client=_MAX_EXAMPLES_PER_CLIENT.value,
      client_epochs_per_round=_CLIENT_EPOCHS_PER_ROUND.value,
      client_shuffle_buffer_size=_CLIENT_SHUFFLE_BUFFER_SIZE.value,
      model_backbone=model_backbone,
      use_client_softmax=_USE_CLIENT_SOFTMAX.value,
      dynamic_clients=_DYNAMIC_CLIENTS.value,
      trainable_conv=_TRAIN_CONV.value)
  logging.info('Task configuration time: %.4f secs.', time.time() - start_time)
  metrics_managers = task.configure_release_managers(_ROOT_OUTPUT_DIR.value,
                                                     _EXPERIMENT_NAME.value)

  if _USE_CLIENT_SOFTMAX.value:
    server_optimizer, client_optimizer, learning_rate_fn = (
        trainer.configure_client_scheduled_optimizers(
            server_learning_rate=_SERVER_LR.value,
            server_momentum=_SERVER_MOMENTUM.value,
            client_learning_rate=_CLIENT_LR.value,
            client_momentum=_CLIENT_MOMENTUM.value,
            client_opt=_CLIENT_OPTIMIZER.value))
    process_type = trainer.ProcessType.FEDPARTIAL
    model_fn = task.embedding_model_fn
  else:
    server_optimizer, client_optimizer = trainer.configure_optimizers(
        server_learning_rate=_SERVER_LR.value,
        server_momentum=_SERVER_MOMENTUM.value,
        client_learning_rate=_CLIENT_LR.value,
        client_momentum=_CLIENT_MOMENTUM.value,
        client_opt=_CLIENT_OPTIMIZER.value)
    process_type = trainer.ProcessType.FEDAVG
    model_fn = task.federated_model_fn
    learning_rate_fn = None
  agg_str = _AGG_TYPE.value
  if agg_str is None:
    aggregator_type = None
  else:
    aggregator_type = trainer.AggregatorType[agg_str.upper()]
  aggregator = trainer.configure_aggregator(
      aggregator_type=aggregator_type,
      model_fn=model_fn,
      clip_norm=_CLIP_NORM.value,
      noise_multiplier=_NOISE_MULTIPLIER.value,
      report_goal=_CLIENTS_PER_ROUND.value,
      target_unclipped_quantile=_UNCLIP_QUANTILE.value)
  process = trainer.build_train_process(
      process_type,
      task=task,
      aggregator=aggregator,
      server_optimizer=server_optimizer,
      client_optimizer=client_optimizer,
      client_learning_rate_fn=learning_rate_fn,
      pretrained_model_path=_PRETRAIN_MODEL_PATH.value,
      head_lr_scale=_HEAD_LR_SCALE.value,
      reconst_iters=_RECONST_ITERS.value,
  )
  eval_fn = trainer.build_eval_fn(
      process_type, train_process=process, task=task)

  export_fn = trainer.build_export_fn(
      process_type, task=task, train_process=process)
  program_state_manager = task.configure_federated_checkpoint_manager(
      _ROOT_OUTPUT_DIR.value, _EXPERIMENT_NAME.value, export_fn,
      _ROUNDS_PER_EXPORT.value)

  if _TOTAL_ROUNDS.value % _ROUNDS_PER_EXPORT.value == 0:
    # Export on the last round will be done by ordinary export in
    # `program_state_manager`, so let final_export_fn be a no-op.
    final_export_fn = lambda state: None
  else:
    final_export_dir = os.path.join(_ROOT_OUTPUT_DIR.value, 'export',
                                    _EXPERIMENT_NAME.value,
                                    'inference_%06d' % _TOTAL_ROUNDS.value)
    final_export_fn = functools.partial(export_fn, export_dir=final_export_dir)

  trainer.train_and_eval(
      task,
      train_process=process,
      evaluation_fn=eval_fn,
      export_fn=final_export_fn,
      total_rounds=_TOTAL_ROUNDS.value,
      clients_per_round=_CLIENTS_PER_ROUND.value,
      rounds_per_eval=_ROUNDS_PER_EVAL.value,
      program_state_manager=program_state_manager,
      rounds_per_saving_program_state=_ROUNDS_PER_CHECKPOINT.value,
      metrics_managers=metrics_managers)
  logging.info('Training completed.')


if __name__ == '__main__':
  flags.mark_flag_as_required('task_type')
  app.run(main)
