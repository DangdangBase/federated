# Copyright 2022, Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r"""This is the python binary for centralized training baselines.

This runs basic centralized trainings using the same model definition and
dataset reading code as the federated training. It is meant to be used for
baseline experiments.

Local debug command:

bazel run run_centralized -- \
  --task_type=emnist --num_steps=10000 --eval_interval_steps=200
"""
import collections
from collections.abc import Sequence
import time

from absl import app
from absl import flags
from absl import logging

from dp_visual_embeddings import centralized_training_loop
from dp_visual_embeddings.models import build_model
from dp_visual_embeddings.tasks import build_task


IRRELEVANT_FLAGS = frozenset(iter(flags.FLAGS))

# Experiment configuration
_EXPERIMENT_NAME = flags.DEFINE_string(
    'experiment_name', 'centralized_embedding',
    'The name of this experiment. Will be'
    'append to  --root_output_dir to separate experiment results. This name is'
    'often generated by XM script for each running task.')
_ROOT_OUTPUT_DIR = flags.DEFINE_string(
    'root_output_dir', '/tmp/', 'Root directory for writing experiment output.')
_REPEAT = flags.DEFINE_integer('repeat', None, 'Repeat index.')

# Training configuration
_TASK_TYPE = flags.DEFINE_enum(
    'task_type', None,
    [task_type.name.lower() for task_type in build_task.get_task_types()],
    'Which task (dataset and model) to use in the experiment.')
_NUM_STEPS = flags.DEFINE_integer('num_steps', 1000,
                                  'Number of optimization steps to execute.')
_BATCH_SIZE = flags.DEFINE_integer('batch_size', 128,
                                   'Training batch size (per GPU/replica).')
_EVAL_BATCH_SIZE = flags.DEFINE_integer('eval_batch_size', 256,
                                        'Batch size used for online eval.')
_SHUFFLE_BUFFER_SIZE = flags.DEFINE_integer(
    'shuffle_buffer_size', 20000,
    'Shuffle buffer used for processing training dataset.')

_MODEL_BACKBONE = flags.DEFINE_enum(
    'model_backbone', 'mobilenet2', build_model.get_backbone_names(),
    'Selects between different convolutional "backbones:" the main part of the '
    'model.')

_CHECKPOINT_INTERVAL_STEPS = flags.DEFINE_integer(
    'checkpoint_interval_steps', 100,
    'The frequency at which to save model weight checkpoints, to allow '
    'resuming if the job is aborted.')

# Eval configuration
_EVAL_INTERVAL_STEPS = flags.DEFINE_integer(
    'eval_interval_steps', 100,
    'The interval at which to evaluate on the validation set.')
_EXPORT_INTERVAL_STEPS = flags.DEFINE_integer(
    'export_interval_steps', 10000,
    'The interval at which to write a `SavedModel` to the export directory.')
_MAX_EVAL_EXAMPLES = flags.DEFINE_integer(
    'max_eval_examples', None,
    'If specified, limits the online eval to only compute metrics using this '
    'many examples or fewer.')

# Optimizers
_INITIAL_LR = flags.DEFINE_float('initial_lr', 1e-3, 'Initial learning rate.')

HPARAM_FLAGS = [f for f in flags.FLAGS if f not in IRRELEVANT_FLAGS]
FLAGS = flags.FLAGS


def _build_flags_hparam_dict():
  """Construct an hparam dictionary from the flags."""
  logging.info('Show FLAGS for debugging:')
  for f in HPARAM_FLAGS:
    logging.info('%s=%s', f, FLAGS[f].value)
  hparam_dict = collections.OrderedDict([
      (name, FLAGS[name].value) for name in HPARAM_FLAGS
  ])
  return hparam_dict


def main(argv: Sequence[str]) -> None:
  if len(argv) > 1:
    raise app.UsageError('Expected no command-line arguments, '
                         'got: {}'.format(argv))

  logging.info('Start training...')
  flags_hparam_dict = _build_flags_hparam_dict()

  task_type_str = _TASK_TYPE.value
  if task_type_str is None:
    raise app.UsageError('Must specify --task_type')
  else:
    task_type = build_task.TaskType[task_type_str.upper()]

  model_backbone = build_model.ModelBackbone[_MODEL_BACKBONE.value.upper()]

  start_time = time.time()
  task = build_task.configure_task(
      task_type=task_type,
      client_batch_size=_BATCH_SIZE.value,
      eval_batch_size=_EVAL_BATCH_SIZE.value,
      train_max_examples_per_client=None,
      eval_max_examples_per_client=_MAX_EVAL_EXAMPLES.value,
      client_shuffle_buffer_size=_SHUFFLE_BUFFER_SIZE.value,
      model_backbone=model_backbone,
      use_client_softmax=False)
  logging.info('Task configuration time: %.4f secs.', time.time() - start_time)

  release_managers = task.configure_release_managers(_ROOT_OUTPUT_DIR.value,
                                                     _EXPERIMENT_NAME.value)
  centralized_training_loop.run_on_task(
      task=task,
      experiment_name=_EXPERIMENT_NAME.value,
      root_output_dir=_ROOT_OUTPUT_DIR.value,
      metrics_managers=release_managers,
      per_replica_batch_size=_BATCH_SIZE.value,
      num_steps=_NUM_STEPS.value,
      checkpoint_interval_steps=_CHECKPOINT_INTERVAL_STEPS.value,
      eval_interval_steps=_EVAL_INTERVAL_STEPS.value,
      export_interval_steps=_EXPORT_INTERVAL_STEPS.value,
      hparams_dict=flags_hparam_dict,
      initial_lr=_INITIAL_LR.value)
  logging.info('Training completed.')


if __name__ == '__main__':
  app.run(main)
